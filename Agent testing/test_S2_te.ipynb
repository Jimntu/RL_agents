{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlagents\n",
    "from mlagents_envs.environment import UnityEnvironment as UE\n",
    "import numpy as np\n",
    "from mlagents_envs.environment import ActionTuple\n",
    "\n",
    "# env =  UE(file_name=\"stage0_160523\\stage0_copy\",seed=1,side_channels=[])\n",
    "env = UE(file_name=r\"C:\\Users\\linzj\\Desktop\\environment\\S2_train\\\\build\",seed=1,side_channels=[],worker_id=8,no_graphics = False)\n",
    "env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linzj\\anaconda3\\envs\\rl\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "vision_output_dim = 3136\n",
    "language_output_dim = 128\n",
    "embedding_dim = 128\n",
    "mixing_dim = 256\n",
    "lstm_hidden_dim = 256\n",
    "num_actions = 4\n",
    "\n",
    "# (3,128,128) --> (64,7,7) = 3136 (3-layer CNN)\n",
    "class VisualModule(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(VisualModule, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=3, padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # self.conv = nn.Sequential(\n",
    "        #     nn.Conv2d(3, 32, kernel_size=5, stride=2, padding=2),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(32, 64, kernel_size=5, stride=2, padding=2),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(128, 64, kernel_size=5, stride=2, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        # )\n",
    "\n",
    "    def forward(self, vt):\n",
    "        encoded_vt = self.conv(vt)\n",
    "        return encoded_vt.view(vt.size(0), -1).squeeze()\n",
    "\n",
    "# one-hot encoding [0 0 1 0 0] --> 128 dimensional embedding (FF)\n",
    "# S1:5 S2:5 S3:11 S4:9 --> 30 + 5 (noun) = 35 in total\n",
    "class LanguageModule(nn.Module): \n",
    "    def __init__(self, embedding_dim):\n",
    "        super(LanguageModule, self).__init__()\n",
    "        self.linear = nn.Linear(512, embedding_dim)\n",
    "          \n",
    "        # self.linear = nn.Sequential(\n",
    "        #     nn.Linear(512, embedding_dim),\n",
    "        #     nn.ReLU())\n",
    "\n",
    "\n",
    "    def forward(self, lt):\n",
    "        embedded_lt = self.linear(lt)\n",
    "        return embedded_lt\n",
    "\n",
    "# 3136(vision) + 128 (language) --> 256 dimensional embedding (FF)\n",
    "class MixingModule(nn.Module):\n",
    "    def __init__(self, vision_output_dim, language_output_dim, mixing_dim):\n",
    "        super(MixingModule, self).__init__()\n",
    "        self.linear = nn.Linear(vision_output_dim + language_output_dim, mixing_dim)\n",
    "        # self.linear = nn.Sequential(\n",
    "        #     nn.Linear(vision_output_dim + language_output_dim, mixing_dim),\n",
    "        #     nn.ReLU())\n",
    "\n",
    "    def forward(self, vision_output, language_output):\n",
    "        combined_output = torch.cat((vision_output, language_output), dim=0)\n",
    "        mixed_output = self.linear(combined_output)\n",
    "        return mixed_output\n",
    "\n",
    "class LSTMModule(nn.Module):\n",
    "    def __init__(self,mixing_dim,lstm_hidden_dim):\n",
    "        super(LSTMModule, self).__init__()\n",
    "        self.lstm = nn.LSTMCell(mixing_dim, lstm_hidden_dim)\n",
    "    \n",
    "    def forward(self,mixed_output,lstm_hidden_state):\n",
    "        lstm_hidden_state = self.lstm(mixed_output, lstm_hidden_state) \n",
    "        # lstm_output = lstm_hidden_state[0] # output is (hidden_state,cell_state), we need hidden state, shape (1,256)\n",
    "        return lstm_hidden_state\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, embedding_dim, vision_output_dim, language_output_dim, mixing_dim, lstm_hidden_dim,num_actions):\n",
    "        super(Agent, self).__init__()\n",
    "        self.language_module = LanguageModule(embedding_dim)\n",
    "        self.visual_module = VisualModule()\n",
    "        self.mixing_module = MixingModule(vision_output_dim, language_output_dim, mixing_dim)\n",
    "        self.lstm_module = LSTMModule(mixing_dim, lstm_hidden_dim)\n",
    "        self.action_predictor = nn.Linear(lstm_hidden_dim, num_actions)\n",
    "        self.value_estimator = nn.Linear(lstm_hidden_dim, 1)\n",
    "\n",
    "    def forward(self, vt, lt, lstm_hidden_state):\n",
    "        vision_output = self.visual_module(vt)\n",
    "        language_output = self.language_module(lt)\n",
    "        mixed_output = self.mixing_module(vision_output, language_output).unsqueeze(0)\n",
    "        lstm_output = self.lstm_module(mixed_output,lstm_hidden_state)\n",
    "        action_probs = self.action_predictor(lstm_output[0]) \n",
    "        value_estimate = self.value_estimator(lstm_output[0])\n",
    "        return action_probs,value_estimate,lstm_output\n",
    "        \n",
    "        \n",
    "    def save(self, episode, ALG_NAME, ENV_ID):\n",
    "        path = os.path.join(r'C:\\Users\\linzj\\Desktop\\model_before', '_'.join([ALG_NAME, ENV_ID]))\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        torch.save(self.state_dict(), os.path.join(path, f'agent_{episode}.pt'))\n",
    "\n",
    "    def load(self, episode, ALG_NAME, ENV_ID):\n",
    "        path = os.path.join(r'C:\\Users\\linzj\\Desktop\\model_before', '_'.join([ALG_NAME, ENV_ID]))\n",
    "        self.load_state_dict(torch.load(os.path.join(path, f'agent_{episode}.pt')))   \n",
    "\n",
    "    \n",
    "    # def load(self, episode, ALG_NAME, ENV_ID):\n",
    "    #     path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n",
    "    #     saved_state_dict = torch.load(os.path.join(path, f'agent_{episode}.pt'))\n",
    "\n",
    "    #     # Create a new state_dict for the model and only copy parameters except 'language_module'\n",
    "    #     new_state_dict = {}\n",
    "    #     for key, value in saved_state_dict.items():\n",
    "    #         if 'language_module' not in key:\n",
    "    #             new_state_dict[key] = value\n",
    "\n",
    "    #     # Load the modified state_dict into the agent\n",
    "    #     self.load_state_dict(new_state_dict, strict=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.0320e-02,  8.2796e-03,  2.1406e-02,  5.6506e-02,  1.2431e-02,\n",
       "        -3.9443e-02, -3.2578e-02, -1.1511e-02, -8.7145e-03,  2.8967e-03,\n",
       "         2.2257e-02, -2.7940e-02,  2.3075e-02, -7.5300e-03,  2.8859e-03,\n",
       "        -5.9377e-03, -1.4408e-01, -3.0802e-03, -2.0899e-02, -5.7119e-02,\n",
       "        -2.8117e-02,  7.1532e-02, -2.0686e-02, -1.0602e-02, -1.7278e-02,\n",
       "        -1.1986e-02, -3.2243e-02,  1.8597e-02,  4.0634e-02, -3.5627e-03,\n",
       "         8.4979e-02, -8.2717e-03,  2.6249e-03, -2.4647e-04,  3.7600e-05,\n",
       "        -1.4175e-02,  3.9522e-02, -8.6989e-02,  2.4306e-02,  2.2828e-02,\n",
       "         1.0826e-02, -2.1569e-02,  2.9212e-02,  3.2010e-03,  3.8441e-02,\n",
       "         5.1095e-02, -4.5703e-02,  1.1612e-02,  4.7064e-03,  2.5812e-03,\n",
       "        -4.5083e-03, -2.7430e-03, -2.2139e-02, -6.6525e-03,  1.9910e-02,\n",
       "        -3.6391e-02, -4.1970e-02,  4.0291e-02, -7.9102e-02, -2.5100e-03,\n",
       "        -8.3486e-02,  3.1922e-02, -1.8551e-02,  9.2341e-03,  1.7096e-02,\n",
       "        -2.4965e-02, -5.1849e-02,  3.5880e-02, -1.0907e-02, -3.3324e-02,\n",
       "        -4.0534e-01, -2.9292e-03, -3.9745e-02,  3.8037e-02,  3.8547e-02,\n",
       "        -8.7966e-02, -1.0326e-01,  7.3742e-03,  2.0493e-02, -1.0047e-02,\n",
       "        -5.2557e-02, -3.0301e-02,  1.3884e-02,  1.5109e-03,  2.8253e-03,\n",
       "        -1.4104e-02, -1.5228e-03,  1.4212e-02,  4.4097e-02, -4.6383e-02,\n",
       "         1.3667e-02, -3.1404e-02, -6.8438e-03, -2.2881e-02, -4.6147e-03,\n",
       "        -2.6771e-02,  2.4418e-02,  2.7364e-02,  5.2985e-03,  2.8198e-02,\n",
       "        -3.9065e-02, -3.9403e-02, -2.9916e-02,  1.5317e-02, -6.8668e-03,\n",
       "        -6.0401e-03, -5.9742e-02,  3.6474e-02, -2.5369e-03,  1.1219e-02,\n",
       "        -2.5935e-04, -1.2893e-02,  7.0349e-03,  1.8859e-02, -2.8079e-03,\n",
       "        -1.2392e-01, -5.3615e-02,  1.1314e-02,  2.8449e-02,  9.9406e-02,\n",
       "        -9.5024e-03, -3.0662e-03, -3.7528e-02,  3.3187e-02, -1.9863e-02,\n",
       "         1.1106e-02,  2.5783e-02, -6.9387e-03, -2.7009e-02, -2.8085e-02,\n",
       "         1.3319e-02,  1.6077e-02,  2.1647e-02, -1.7845e-02, -3.2907e-02,\n",
       "        -9.7633e-02,  3.6429e-02, -1.3722e-02, -7.8927e-04, -3.2281e-02,\n",
       "         1.0131e-02,  3.5946e-02,  1.1898e-03,  1.0073e-02, -3.1251e-02,\n",
       "         2.5894e-02,  2.6882e-02,  3.4757e-02,  5.5477e-03, -7.8397e-03,\n",
       "        -2.7693e-03, -2.9528e-02,  9.4115e-02, -1.1135e-02,  1.8448e-02,\n",
       "         1.3619e-02,  6.4704e-02,  1.8566e-02, -2.8741e-02,  2.2823e-02,\n",
       "        -2.0004e-02,  2.9603e-02,  6.7801e-03, -3.0802e-02, -2.2113e-02,\n",
       "         4.2565e-02,  1.9213e-02, -8.6181e-02, -3.6261e-02, -9.3336e-03,\n",
       "         5.5028e-02, -1.2828e-02, -6.0333e-03,  1.0935e-02,  3.1506e-02,\n",
       "         1.3940e-02,  3.8872e-04, -5.0377e-03,  1.3806e-02, -3.4513e-02,\n",
       "         5.5182e-02, -3.0319e-02, -2.8457e-02,  6.6615e-04,  5.4386e-04,\n",
       "         1.6029e-02, -1.2337e-03,  1.6769e-02, -2.2656e-02,  1.3501e-02,\n",
       "         7.5048e-03, -9.8118e-02, -4.9385e-02,  1.8733e-03,  1.1849e-03,\n",
       "         1.1858e-02,  1.0916e-02, -1.4276e-02, -4.7960e-02,  1.6606e-02,\n",
       "        -1.8286e-02, -1.3172e-02, -5.4572e-02,  4.8524e-02, -4.6706e-02,\n",
       "         6.8214e-03, -1.0067e-03,  3.4395e-02,  5.0914e-02, -1.7813e-02,\n",
       "        -1.6076e-02,  4.4509e-02, -5.1133e-02,  2.1298e-02, -3.5949e-02,\n",
       "         1.0790e-02,  3.8048e-02, -2.1589e-02,  2.8308e-02, -9.0073e-02,\n",
       "         2.9534e-02,  1.6148e-03, -2.5937e-02,  2.9707e-02, -5.3505e-02,\n",
       "        -2.0991e-02, -1.9327e-02, -1.4755e-02,  3.8160e-02, -4.5845e-02,\n",
       "         6.6639e-03, -2.2223e-02,  3.5333e-02,  1.2862e-02,  1.1146e-02,\n",
       "        -2.4776e-03,  4.3178e-03,  1.2825e-02,  6.5159e-02, -1.0931e-02,\n",
       "        -8.0760e-02, -4.1533e-02,  2.0174e-02, -1.8358e-02, -7.6448e-03,\n",
       "         2.5291e-03,  2.9948e-03, -2.2110e-02,  9.7147e-02,  1.1244e-02,\n",
       "        -3.1946e-02, -5.7845e-03,  1.0448e-02, -4.0519e-01, -3.4388e-02,\n",
       "         4.9871e-02,  3.3652e-03,  3.6991e-02, -5.2808e-02,  4.7662e-02,\n",
       "        -1.2020e-02, -3.1112e-02, -1.4577e-02, -4.0893e-03,  1.7658e-02,\n",
       "        -6.3051e-02,  2.5976e-03,  9.7483e-03,  3.6309e-02,  1.0139e-02,\n",
       "         2.3856e-02, -4.7522e-02, -1.1663e-02, -1.7771e-03,  1.2019e-02,\n",
       "         2.4713e-02,  2.8651e-02, -1.1393e-02,  1.2206e-02, -5.6273e-02,\n",
       "         4.9102e-03,  3.7181e-02,  5.4073e-02, -1.2850e-02, -6.3566e-02,\n",
       "        -3.6361e-02, -2.5691e-02, -3.4764e-03,  3.6760e-02, -4.2018e-03,\n",
       "        -8.4188e-02,  1.1713e-02,  1.9626e-02,  7.4006e-03, -2.1885e-02,\n",
       "         2.7628e-02, -2.7389e-02, -3.5733e-02, -3.3643e-02, -1.4719e-02,\n",
       "        -1.1058e-02, -1.1488e-02, -2.2508e-02,  8.8267e-04, -5.9624e-02,\n",
       "         1.2059e-02,  3.3569e-02, -9.3984e-02,  7.1866e-03,  3.2847e-02,\n",
       "         2.9603e-02, -1.2906e-01, -7.4741e-03,  3.3723e-02, -4.4310e-02,\n",
       "         5.6179e-03, -2.1803e-02, -1.9502e-03, -3.5834e-02,  2.7997e-02,\n",
       "        -2.3803e-02, -3.1702e-03, -1.2632e-02,  1.2128e-02, -2.1444e-02,\n",
       "        -4.2306e-03, -1.1807e-01, -9.6539e-03, -3.3009e-03,  1.6006e-02,\n",
       "         2.9870e-03,  2.5481e-02, -6.4812e-03,  4.5655e-03, -3.2222e-02,\n",
       "         1.7007e-02, -3.1166e-02, -2.0634e-02, -1.4089e-02, -1.7430e-02,\n",
       "         3.0598e-02,  1.4343e-02,  1.0443e-02,  9.9390e-03, -2.4364e-02,\n",
       "        -1.6971e-02, -3.1674e-03, -3.2811e-02, -2.3423e-02, -1.3985e-01,\n",
       "         4.1669e-02, -4.0937e-03,  5.2592e-02, -5.0620e-02, -1.8866e-02,\n",
       "         4.9081e-03, -8.3535e-03, -1.4560e-02, -4.5597e-02,  4.3253e-02,\n",
       "        -2.1844e-02,  3.1836e-02, -6.2141e-02,  3.2403e-02, -1.3455e-02,\n",
       "         2.8544e-02,  1.7915e-02, -6.8500e-03, -4.2864e-02,  2.1618e-02,\n",
       "        -9.4880e-03, -1.7949e-03,  1.6510e-02,  4.5754e-02, -5.1349e-02,\n",
       "        -5.6366e-02, -6.5180e-02, -1.5139e-02, -8.0818e-04, -1.2675e-02,\n",
       "         4.1763e-02, -5.4623e-02,  1.5388e-02, -2.4594e-02,  1.1586e-02,\n",
       "        -5.4786e-02, -1.2546e-03,  7.3419e-03,  2.7450e-04, -1.5687e-02,\n",
       "         9.9080e-03,  5.9779e-03,  3.5965e-02, -1.3911e-02, -7.1669e-02,\n",
       "        -1.4652e-03, -1.1335e-01,  9.4368e-02, -7.5983e-02, -1.4365e-02,\n",
       "         4.3468e-02, -4.0670e-02,  7.0750e-02,  1.2376e-01,  1.4761e-02,\n",
       "         1.5510e-02,  9.1705e-03, -2.3593e-02, -5.0520e-03, -7.6906e-03,\n",
       "         5.8050e-02, -2.0640e-02, -3.4560e-02,  2.3231e-02, -8.5990e-03,\n",
       "         3.3484e-02, -3.4775e-02, -1.0778e-02,  6.4987e-03, -2.2661e-02,\n",
       "        -2.8980e-02,  8.2578e-03, -2.6102e-02, -3.2242e-03, -5.5605e-02,\n",
       "        -1.9217e-02, -6.0189e-02, -1.0861e-02,  6.7351e-02,  4.9204e-02,\n",
       "        -1.3768e-02, -1.8096e-02,  1.6197e-02,  7.3881e-03,  4.2929e-02,\n",
       "        -4.7922e-02, -2.7253e-03, -2.3367e-02, -3.5821e-02, -6.0172e-03,\n",
       "        -1.9934e-02,  1.7644e-02,  2.8734e-02, -9.6259e-03, -3.7710e-02,\n",
       "        -1.2295e-02, -1.4801e-02, -4.1772e-03, -1.1762e-01, -1.1704e-02,\n",
       "         2.5965e-02,  3.4789e-02,  5.0877e-03,  2.6954e-02, -9.1394e-03,\n",
       "        -1.1541e-01,  6.2354e-03,  2.6490e-02, -3.9876e-02, -2.3095e-03,\n",
       "        -3.6240e-02, -1.5944e-03, -3.8872e-02,  3.8543e-02,  5.1797e-03,\n",
       "        -2.8906e-04,  3.3617e-02, -6.2243e-02, -4.7609e-03, -7.7426e-03,\n",
       "         1.1471e-02,  3.2248e-02, -1.0107e-02, -1.9446e-02, -3.1021e-02,\n",
       "        -4.5046e-02, -1.7444e-02, -1.4929e-02, -2.0344e-02,  2.2697e-02,\n",
       "         8.3683e-03,  1.3038e-02, -6.8374e-04,  1.3737e-02,  2.2274e-03,\n",
       "        -4.8263e-02, -1.2039e-04, -5.1270e-02, -3.1885e-02,  2.7119e-02,\n",
       "         3.0675e-02, -1.3183e-02,  5.6985e-03, -1.1834e-02,  1.4611e-03,\n",
       "        -7.3121e-02,  7.5512e-03, -5.2358e-02, -2.0530e-02, -3.8258e-02,\n",
       "         2.4058e-02, -1.1819e-02,  5.5024e-02,  4.4115e-02, -5.6341e-02,\n",
       "        -5.3619e-03, -4.0177e-02,  6.1235e-02,  4.4804e-02,  1.0299e-02,\n",
       "         1.2408e-02,  3.7763e-03])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "input_string = 'green cube'\n",
    "# Load the dictionary from the pickle file\n",
    "with open(r'C:\\Users\\linzj\\Desktop\\Agent training\\clip.pkl', 'rb') as pickle_file:\n",
    "    clip_encoder = pickle.load(pickle_file)\n",
    "clip_encoder[input_string][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.0364e-04, -1.3565e-02, -8.4915e-03, -2.4780e-02,  5.0385e-02,\n",
       "         -4.3091e-02, -1.5945e-02, -9.7290e-02, -4.2114e-02, -5.6763e-03,\n",
       "          1.7502e-02, -1.0849e-02, -3.0914e-02, -1.3523e-03,  2.5803e-02,\n",
       "          1.8066e-02,  1.2726e-02, -2.0294e-02, -5.1544e-02, -1.3361e-03,\n",
       "          3.9307e-02, -1.8967e-02,  5.2032e-02, -1.7975e-02, -1.4137e-02,\n",
       "          1.7151e-02,  1.4982e-03,  2.4033e-02, -6.2675e-03,  1.7334e-02,\n",
       "         -1.3496e-02,  3.1185e-03,  1.6617e-02,  8.6288e-03,  7.2266e-02,\n",
       "         -2.0325e-04,  2.9419e-02,  1.0895e-02,  2.5360e-02, -1.0521e-02,\n",
       "          3.6987e-02,  3.0670e-03, -1.3016e-02, -2.4277e-02,  6.8398e-03,\n",
       "          9.0256e-03, -6.8787e-02,  3.4973e-02,  2.8259e-02,  3.2684e-02,\n",
       "         -4.0009e-02, -6.1066e-02,  2.0370e-02, -1.1147e-02,  1.1612e-02,\n",
       "          2.1301e-02, -1.4069e-02, -1.9882e-02, -1.6556e-03, -1.5030e-03,\n",
       "          6.5994e-03,  4.2038e-03, -2.3880e-02, -6.7568e-04,  4.7188e-03,\n",
       "         -7.8583e-03,  3.1982e-02,  5.0583e-03,  2.5238e-02,  1.5099e-02,\n",
       "         -1.0979e-02, -4.6295e-02,  1.1879e-02,  2.1267e-03, -5.0720e-02,\n",
       "          2.3163e-02, -6.3705e-03, -5.1636e-02,  2.0199e-03, -1.6495e-02,\n",
       "         -4.9667e-03,  6.0455e-02, -3.1555e-02,  3.0472e-02, -1.0529e-02,\n",
       "         -5.5580e-03,  3.8757e-02, -5.3223e-02,  4.2801e-03,  7.5493e-03,\n",
       "         -1.7853e-02,  2.0035e-02, -1.4661e-01, -5.8929e-02, -3.4912e-02,\n",
       "          4.4823e-04,  6.3721e-02,  5.8044e-02, -6.2347e-02, -2.8427e-02,\n",
       "          2.3544e-02,  6.4087e-02,  3.9673e-02, -2.1759e-02, -6.6589e-02,\n",
       "         -2.3010e-02, -1.7929e-03, -2.8443e-04, -1.6968e-02,  1.2428e-02,\n",
       "          1.7014e-03,  6.2523e-03, -7.7019e-03, -7.4829e-02, -2.3254e-02,\n",
       "         -6.8741e-03, -1.4168e-02,  6.4964e-03,  9.6893e-03,  2.2659e-02,\n",
       "          1.6665e-04, -1.8997e-02,  3.6133e-02, -2.2156e-02, -1.3054e-02,\n",
       "          1.9180e-02,  6.9771e-03, -1.6159e-02,  5.6114e-03,  8.5083e-02,\n",
       "          3.8361e-02,  7.6675e-03,  1.5656e-02,  4.9634e-01,  1.0017e-02,\n",
       "         -2.4734e-02,  1.6108e-03, -7.4524e-02, -2.0813e-02, -2.4368e-02,\n",
       "          1.3657e-02, -1.1238e-02, -2.7481e-02, -6.9618e-03, -2.1423e-02,\n",
       "         -3.8361e-02,  2.3384e-03,  4.8828e-03, -2.9999e-02,  2.1576e-02,\n",
       "          1.4122e-02,  2.1225e-02, -6.5851e-04,  3.0701e-02, -5.2490e-02,\n",
       "         -2.6947e-02, -4.4861e-02,  1.3304e-03, -2.2259e-03, -7.7782e-03,\n",
       "          2.1255e-02, -7.8964e-03,  1.4679e-02, -3.1013e-03, -1.2329e-02,\n",
       "          1.7593e-02,  8.9111e-03, -2.5082e-03,  2.7771e-02, -2.0905e-02,\n",
       "         -6.3553e-03, -8.3208e-04,  1.1032e-02, -1.3908e-02, -1.8860e-02,\n",
       "          1.4595e-02, -3.6743e-02, -2.8992e-02, -2.8336e-02, -8.9493e-03,\n",
       "          7.8659e-03,  2.4643e-03,  1.6739e-02, -2.0920e-02, -3.2104e-02,\n",
       "         -3.7079e-02, -2.5070e-02, -9.8267e-03, -1.8051e-02, -4.0283e-02,\n",
       "          1.1299e-02, -1.7899e-02,  3.8422e-02, -2.7039e-02, -1.8784e-02,\n",
       "          2.5070e-02, -2.6932e-02,  3.0853e-02,  1.0269e-02, -2.8946e-02,\n",
       "         -1.5221e-02, -3.5248e-02, -5.2376e-03,  2.9526e-02,  8.7662e-03,\n",
       "          2.1255e-02,  5.7297e-03,  2.8061e-02,  2.2888e-02, -3.6804e-02,\n",
       "          1.6861e-02,  8.5526e-03, -1.0544e-02,  2.2369e-02, -1.8219e-02,\n",
       "         -1.3222e-02,  4.1992e-02, -1.5991e-02,  3.6163e-02, -1.9531e-02,\n",
       "          7.4816e-04, -2.7939e-02, -4.9255e-02, -5.7922e-02,  3.0716e-02,\n",
       "          2.7603e-02, -9.2697e-03, -6.1493e-02, -1.2611e-02,  1.5442e-02,\n",
       "         -4.2786e-02,  3.7079e-02,  2.2186e-02, -9.1553e-02, -2.5650e-02,\n",
       "          7.5317e-02, -2.5024e-02, -1.8982e-02, -2.8336e-02,  6.8359e-03,\n",
       "         -8.9569e-03,  3.3386e-02,  4.5685e-02,  2.1942e-02,  1.6953e-02,\n",
       "          4.8950e-02, -6.1989e-04,  2.1759e-02,  1.4877e-03, -3.0746e-02,\n",
       "         -5.6213e-02, -1.6647e-02,  1.5541e-02,  4.6387e-02, -3.1372e-02,\n",
       "         -5.4893e-03,  7.5562e-02, -3.0556e-03,  5.6725e-03, -2.9182e-03,\n",
       "         -1.9257e-02, -2.5574e-02,  2.6566e-02,  2.0157e-02,  4.8943e-03,\n",
       "          1.0529e-03,  1.6830e-02, -5.1300e-02, -3.3295e-02, -7.4425e-03,\n",
       "         -2.5436e-02,  1.3723e-03,  4.6158e-03, -2.1835e-02, -4.6997e-03,\n",
       "          3.7750e-02, -6.9466e-03, -1.1749e-03, -1.2764e-02, -8.0261e-03,\n",
       "          1.2848e-02,  1.5976e-02,  2.0084e-03,  3.3112e-02,  4.7951e-03,\n",
       "          6.5735e-02,  1.6052e-02,  5.2307e-02,  2.1530e-02, -1.4511e-02,\n",
       "         -1.8539e-02, -3.6926e-02,  1.0162e-02, -1.8051e-02,  2.0386e-02,\n",
       "         -2.5311e-03,  2.6123e-02,  3.4294e-03,  3.0411e-02,  1.6113e-02,\n",
       "         -1.2924e-02, -3.9948e-02, -2.4185e-03,  3.1342e-02,  1.3573e-02,\n",
       "          1.1162e-02,  1.7080e-03,  3.5187e-02,  6.4697e-02,  1.5235e-04,\n",
       "         -6.2408e-03, -1.9028e-02,  4.9634e-01,  2.8366e-02,  1.2535e-02,\n",
       "          1.0643e-02,  3.0243e-02,  3.1021e-02,  4.1412e-02,  4.3535e-04,\n",
       "         -2.3212e-03,  4.8279e-02,  1.1696e-02, -1.6815e-02, -2.2144e-03,\n",
       "          5.4207e-03, -7.0305e-03,  6.4316e-03, -2.1179e-02, -2.0264e-01,\n",
       "          2.3232e-03, -1.2337e-02,  7.1411e-02,  1.1528e-02,  1.1719e-02,\n",
       "          7.3395e-03,  4.2297e-02,  2.3270e-02,  1.9119e-02, -6.1096e-02,\n",
       "         -2.5238e-02, -2.1408e-02,  3.2715e-02, -1.5879e-03,  4.0833e-02,\n",
       "          2.7740e-02, -1.6251e-02, -1.3075e-03, -1.9806e-02,  1.5869e-02,\n",
       "          4.2358e-02, -1.5793e-02,  5.8441e-03,  1.7044e-02,  1.3489e-02,\n",
       "         -2.2064e-02, -2.0432e-02,  2.7573e-02, -1.8036e-02, -4.3793e-02,\n",
       "         -1.3786e-02, -3.7292e-02,  3.5309e-02, -5.0232e-02,  2.8172e-03,\n",
       "         -2.2949e-02,  1.4969e-02,  1.4481e-02,  2.9282e-02, -3.6621e-02,\n",
       "         -3.7079e-02, -1.9424e-02,  6.0959e-03, -6.4850e-05, -2.5864e-02,\n",
       "         -4.3106e-03, -2.4231e-02,  1.3519e-02, -1.1978e-02,  2.9724e-02,\n",
       "          2.2430e-02, -3.6068e-03,  4.8981e-02,  2.5635e-02, -1.0780e-02,\n",
       "          1.3542e-02,  4.7241e-02,  3.7598e-02, -2.3865e-02,  1.3962e-02,\n",
       "         -7.3669e-02,  4.7989e-03, -6.0692e-03,  2.8152e-02, -1.2383e-02,\n",
       "          3.9551e-02,  2.0706e-02, -6.8741e-03, -5.7373e-03,  5.7037e-02,\n",
       "         -5.1117e-02,  5.2277e-02, -9.4147e-03, -4.3678e-03, -6.2988e-02,\n",
       "          4.0070e-02, -4.8126e-02, -2.8549e-02, -2.1439e-02, -3.0136e-02,\n",
       "         -2.4933e-02,  2.7725e-02, -2.9640e-03, -2.8946e-02,  3.1403e-02,\n",
       "         -3.5675e-02, -6.8298e-02,  1.4412e-02, -7.7486e-04, -1.1406e-02,\n",
       "          1.2007e-03,  2.0905e-02,  4.3396e-02, -1.0217e-01,  5.6801e-03,\n",
       "          2.8854e-02, -3.3478e-02, -2.7298e-02,  6.3019e-03, -1.1230e-02,\n",
       "         -1.0948e-02, -4.9042e-02, -2.4826e-02,  4.9530e-02, -1.7853e-02,\n",
       "          2.4628e-02,  8.2245e-03,  1.8494e-02, -1.6312e-02, -2.4063e-02,\n",
       "          3.4576e-02, -1.2878e-02, -2.9861e-02,  3.0441e-02,  1.4782e-03,\n",
       "         -6.1684e-03, -9.9640e-03, -1.2047e-02, -5.2376e-03,  6.7810e-02,\n",
       "         -3.6144e-03, -9.3460e-03,  2.2446e-02,  3.0289e-03,  2.1194e-02,\n",
       "          1.1803e-02,  3.8208e-02,  6.1157e-02,  3.6255e-02,  5.9700e-03,\n",
       "          3.7422e-03,  2.6932e-02, -1.3954e-02,  6.1096e-02,  1.6495e-02,\n",
       "         -1.4563e-03,  3.6407e-02, -8.7433e-03,  9.1705e-03,  5.8975e-03,\n",
       "          1.9455e-02, -6.6956e-02, -6.2981e-03,  1.6830e-02, -2.4624e-03,\n",
       "         -7.0915e-03, -2.8687e-02, -2.5360e-02,  2.2079e-02, -2.3346e-02,\n",
       "         -7.5684e-03, -6.3904e-02,  1.2589e-02, -4.2816e-02,  1.6312e-02,\n",
       "         -2.0111e-02,  4.8065e-02,  5.0232e-02,  1.9608e-03,  9.8724e-03,\n",
       "          1.4496e-02, -4.4586e-02,  2.5070e-02,  1.0059e-01,  2.1042e-02,\n",
       "          4.0558e-02,  2.2278e-02, -1.6602e-02, -1.0269e-02,  4.3068e-03,\n",
       "         -8.2779e-03, -4.8462e-02,  2.7817e-02,  2.9011e-03,  9.7656e-02,\n",
       "         -2.6703e-03, -2.2461e-02, -6.0081e-03,  3.0701e-02,  1.5535e-03,\n",
       "         -2.4048e-02,  3.9917e-02]], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import clip, open_clip\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "text = clip.tokenize(input_string).to(device)\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x77 and 512x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-96711231249a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     54\u001b[0m                 \u001b[0mlstm_hidden_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlstm_hidden_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m                 \u001b[0mSTEPS\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m                 \u001b[0mpolicy_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlstm_hidden_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlstm_hidden_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m                 \u001b[1;31m# value = value.detach()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m                 \u001b[0mdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy_dist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\linzj\\anaconda3\\envs\\rl\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-ff31a88c3398>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, vt, lt, lstm_hidden_state)\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlstm_hidden_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[0mvision_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvisual_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[0mlanguage_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlanguage_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m         \u001b[0mmixed_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmixing_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvision_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[0mlstm_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmixed_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlstm_hidden_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\linzj\\anaconda3\\envs\\rl\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-ff31a88c3398>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, lt)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0membedded_lt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0membedded_lt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\linzj\\anaconda3\\envs\\rl\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\linzj\\anaconda3\\envs\\rl\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\linzj\\anaconda3\\envs\\rl\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x77 and 512x128)"
     ]
    }
   ],
   "source": [
    "#-1(20000): -5 # -2.5 (10000): -6.73  #-5 (5000):-9.46  # -7.5 (5000): -7.02  #-10 (5000):-10(circle same place)\n",
    "# S1_4:12996      S1_13:6495           S1_14:4587        S1_15: 4073            S1_16:3085\n",
    "from torch.distributions import Categorical\n",
    "import open_clip\n",
    "device = torch.device(\"cpu\")\n",
    "tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "speed = 3\n",
    "MAX_STEPS = 500\n",
    "TEST_EPISODES = 100\n",
    "ALG_NAME = 'S2clip_final'\n",
    "ENV_ID = '2' \n",
    "episode = 16595\n",
    "tracked_agent = -1\n",
    "agent = Agent(embedding_dim, vision_output_dim, language_output_dim, mixing_dim, lstm_hidden_dim,num_actions)\n",
    "agent.load(episode,ALG_NAME,ENV_ID)\n",
    "average = 0\n",
    "hashmap = {\n",
    "0: 'capsule',\n",
    "1: 'cube',\n",
    "2: 'cylinder',\n",
    "3: 'prism',\n",
    "4: 'sphere',\n",
    "5: 'red',\n",
    "6: 'green',\n",
    "7: 'blue',\n",
    "8: 'yellow',\n",
    "9: 'black'}\n",
    "for episode in range(TEST_EPISODES):\n",
    "            STEPS = 0\n",
    "            episode_reward = 0\n",
    "            behavior_name=list(env.behavior_specs)[0]\n",
    "            spec=env.behavior_specs[behavior_name]\n",
    "            # state = env.reset().astype(np.float32)\n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "            # state -- vt, lt, lstm\n",
    "            vt = torch.tensor(decision_steps.obs[0]).reshape(1,3,128,128).to(device)\n",
    "            index1 = int(decision_steps.obs[1][0][0])\n",
    "            index2 = int(decision_steps.obs[1][0][1])+5\n",
    "            input_string = f'{hashmap[index2]} {hashmap[index1]}'\n",
    "            # print(f'TEST: ---{input_string}---')\n",
    "            lt = tokenizer(input_string)\n",
    "            lstm_hidden_state = (torch.zeros(1, lstm_hidden_dim).to(device), torch.zeros(1, lstm_hidden_dim).to(device))\n",
    "            done = False\n",
    "            while True:\n",
    "\n",
    "                # Need to use when calculating the loss\n",
    "                log_probs = []\n",
    "                # values = []\n",
    "                values = torch.empty(0).to(device)\n",
    "                rewards = []\n",
    "\n",
    "                \n",
    "                lstm_hidden_state = tuple(tensor.detach() for tensor in lstm_hidden_state)\n",
    "                STEPS += 1\n",
    "                policy_dist, value, lstm_hidden_state = agent(vt,lt,lstm_hidden_state)\n",
    "                # value = value.detach()\n",
    "                dist = F.softmax(policy_dist.detach(),dim=1).cpu().numpy()\n",
    "                \n",
    "\n",
    "                action_dist = Categorical(F.softmax(policy_dist.detach(),dim=1))\n",
    "                # action_dist = Categorical(F.softmax(policy_dist,dim=1))\n",
    "                action = action_dist.sample() # sample an action from action_dist\n",
    "                action_onehot = F.one_hot(torch.tensor(action),num_actions).cpu()\n",
    "                \n",
    "                log_prob = torch.log(F.softmax(policy_dist,dim=1)[0][action])\n",
    "                # log_prob = torch.log(F.softmax(policy_dist,dim=1)[0][action])\n",
    "                # entropy = -np.sum(np.mean(dist)* np.log(dist))\n",
    "                entropy = F.cross_entropy(policy_dist.detach(), action)\n",
    "\n",
    "                discrete_actions = np.array(action_onehot).reshape(1,4)*speed\n",
    "                action_tuple = ActionTuple()\n",
    "                action_tuple.add_discrete(discrete_actions)\n",
    "                env.set_actions(behavior_name,action_tuple)\n",
    "                env.step()\n",
    "                decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "\n",
    "                if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "                    tracked_agent = decision_steps.agent_id[0]\n",
    "                    # print(tracked_agent)\n",
    "\n",
    "                if tracked_agent in terminal_steps: # roll over or hit the target\n",
    "                    # print('TEST: Agent in terminal steps')\n",
    "                    done = True\n",
    "                    reward = terminal_steps[tracked_agent].reward\n",
    "                    if reward > 0:\n",
    "                        pass\n",
    "                    else: reward = -1 # roll over or other unseen conditions\n",
    "\n",
    "                    # print(f'TEST: Terminal Step reward: {reward}')\n",
    "\n",
    "                elif tracked_agent in decision_steps: # the agent which requires action\n",
    "                    reward = decision_steps[tracked_agent].reward\n",
    "                    # print(f'Decision Step reward: {reward}')\n",
    "                    # if reward<0:\n",
    "                    #     print(f'TEST: Decision Step reward: {reward}')\n",
    "                if STEPS >= MAX_STEPS:\n",
    "                    reward = -10\n",
    "                    # print(f'TEST: Max Step Reward: {reward}')\n",
    "                    env.reset()\n",
    "                    done = True\n",
    "                # if STEPS % 100 == 0:\n",
    "                #     print (f'TEST: Step: {STEPS}')\n",
    "\n",
    "                episode_reward = episode_reward + reward\n",
    "\n",
    "                rewards.append(reward)\n",
    "                # values.append(value)\n",
    "                values = torch.cat((values, value), dim=0)\n",
    "                log_probs.append(log_prob)\n",
    "                entropy_term = entropy_term + entropy\n",
    "                vt_new = torch.tensor(decision_steps.obs[0]).reshape(1,3,128,128).to(device)\n",
    "                vt = vt_new\n",
    "            average += episode_reward / TEST_EPISODES\n",
    "            print(f'Episode: {episode}, Episode reward: {episode_reward}')\n",
    "print(f'Average Episode Reward: {average}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
