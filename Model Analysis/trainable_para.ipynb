{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "vision_output_dim = 3136\n",
    "num_words = 44  # Number of unique words in the vocabulary\n",
    "language_output_dim = 128\n",
    "embedding_dim = 128\n",
    "mixing_dim = 256\n",
    "lstm_hidden_dim = 256\n",
    "num_actions = 4\n",
    "\n",
    "# (3,128,128) --> (64,7,7) = 3136 (3-layer CNN)\n",
    "class VisualModule(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(VisualModule, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=3, padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # self.conv = nn.Sequential(\n",
    "        #     nn.Conv2d(3, 32, kernel_size=5, stride=2, padding=2),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(32, 64, kernel_size=5, stride=2, padding=2),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(128, 64, kernel_size=5, stride=2, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        # )\n",
    "\n",
    "    def forward(self, vt):\n",
    "        encoded_vt = self.conv(vt)\n",
    "        return encoded_vt.view(vt.size(0), -1).squeeze()\n",
    "\n",
    "# one-hot encoding [0 0 1 0 0] --> 128 dimensional embedding (FF)\n",
    "# S1:5 S2:5 S3:11 S4:9 --> 30 + 5 (noun) = 35 in total\n",
    "class LanguageModule(nn.Module): \n",
    "    def __init__(self, num_words, embedding_dim):\n",
    "        super(LanguageModule, self).__init__()\n",
    "        self.embedding = nn.Linear(num_words, embedding_dim)\n",
    "\n",
    "    def forward(self, lt):\n",
    "        embedded_lt = self.embedding(lt)\n",
    "        return embedded_lt\n",
    "\n",
    "# 3136(vision) + 128 (language) --> 256 dimensional embedding (FF)\n",
    "class MixingModule(nn.Module):\n",
    "    def __init__(self, vision_output_dim, language_output_dim, mixing_dim):\n",
    "        super(MixingModule, self).__init__()\n",
    "        self.linear = nn.Linear(vision_output_dim + language_output_dim, mixing_dim)\n",
    "\n",
    "    def forward(self, vision_output, language_output):\n",
    "        combined_output = torch.cat((vision_output, language_output), dim=0)\n",
    "        mixed_output = self.linear(combined_output)\n",
    "        return mixed_output\n",
    "\n",
    "class LSTMModule(nn.Module):\n",
    "    def __init__(self,mixing_dim,lstm_hidden_dim):\n",
    "        super(LSTMModule, self).__init__()\n",
    "        self.lstm = nn.LSTMCell(mixing_dim, lstm_hidden_dim)\n",
    "    \n",
    "    def forward(self,mixed_output,lstm_hidden_state):\n",
    "        lstm_hidden_state = self.lstm(mixed_output, lstm_hidden_state) \n",
    "        # lstm_output = lstm_hidden_state[0] # output is (hidden_state,cell_state), we need hidden state, shape (1,256)\n",
    "        return lstm_hidden_state\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, num_words, embedding_dim, vision_output_dim, language_output_dim, mixing_dim, lstm_hidden_dim,num_actions):\n",
    "        super(Agent, self).__init__()\n",
    "        self.language_module = LanguageModule(num_words, embedding_dim)\n",
    "        self.visual_module = VisualModule()\n",
    "        self.mixing_module = MixingModule(vision_output_dim, language_output_dim, mixing_dim)\n",
    "        self.lstm_module = LSTMModule(mixing_dim, lstm_hidden_dim)\n",
    "        self.action_predictor = nn.Linear(lstm_hidden_dim, num_actions)\n",
    "        self.value_estimator = nn.Linear(lstm_hidden_dim, 1)\n",
    "\n",
    "    def forward(self, vt, lt, lstm_hidden_state):\n",
    "        vision_output = self.visual_module(vt)\n",
    "        language_output = self.language_module(lt)\n",
    "        mixed_output = self.mixing_module(vision_output, language_output).unsqueeze(0)\n",
    "        lstm_output = self.lstm_module(mixed_output,lstm_hidden_state)\n",
    "        action_probs = self.action_predictor(lstm_output[0]) \n",
    "        value_estimate = self.value_estimator(lstm_output[0])\n",
    "        return action_probs,value_estimate,lstm_output\n",
    "        \n",
    "        \n",
    "    def save(self, episode, ALG_NAME, ENV_ID):\n",
    "        path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        torch.save(self.state_dict(), os.path.join(path, f'agent_{episode}.pt'))\n",
    "\n",
    "    def load(self, episode, ALG_NAME, ENV_ID):\n",
    "        path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n",
    "        self.load_state_dict(torch.load(os.path.join(path, f'agent_{episode}.pt')))     \n",
    "\n",
    "    def count_trainable_parameters(self):\n",
    "        print(\"Trainable parameters in each module:\")\n",
    "        for name, module in self.named_modules():\n",
    "            total_params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "            print(f\"{name}: {total_params}\")\n",
    "    # def load(self, episode, ALG_NAME, ENV_ID):\n",
    "    #     path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n",
    "    #     saved_state_dict = torch.load(os.path.join(path, f'agent_{episode}.pt'))\n",
    "\n",
    "    #     # Create a new state_dict for the model and only copy parameters except 'language_module'\n",
    "    #     new_state_dict = {}\n",
    "    #     for key, value in saved_state_dict.items():\n",
    "    #         if 'language_module' not in key:\n",
    "    #             new_state_dict[key] = value\n",
    "\n",
    "    #     # Load the modified state_dict into the agent\n",
    "    #     self.load_state_dict(new_state_dict, strict=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters in each module:\n",
      ": 1425541\n",
      "language_module: 5760\n",
      "language_module.embedding: 5760\n",
      "visual_module: 56320\n",
      "visual_module.conv: 56320\n",
      "visual_module.conv.0: 896\n",
      "visual_module.conv.1: 0\n",
      "visual_module.conv.2: 18496\n",
      "visual_module.conv.3: 0\n",
      "visual_module.conv.4: 36928\n",
      "visual_module.conv.5: 0\n",
      "mixing_module: 835840\n",
      "mixing_module.linear: 835840\n",
      "lstm_module: 526336\n",
      "lstm_module.lstm: 526336\n",
      "action_predictor: 1028\n",
      "value_estimator: 257\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(num_words, embedding_dim, vision_output_dim, language_output_dim, mixing_dim, lstm_hidden_dim,num_actions)\n",
    "agent.count_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "num_words = 44  # Number of unique words in the vocabulary\n",
    "lstm_hidden_dim = 256\n",
    "num_actions = 4\n",
    "\n",
    "# (3,128,128) --> (64,16,16)\n",
    "class ImageEncoderCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageEncoderCNN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=3, padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc = nn.Linear(64 * 7* 7, 256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = F.relu(self.fc(x))\n",
    "        return x\n",
    "\n",
    "class LanguageEncoderMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LanguageEncoderMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.fc1(x)\n",
    "        return x1\n",
    "\n",
    "class KVLinear(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(KVLinear, self).__init__()\n",
    "        self.key = nn.Linear(hidden_size, hidden_size)\n",
    "        self.value = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.key(x)\n",
    "        values = self.value(x)\n",
    "        return keys, values\n",
    "\n",
    "class QLinear(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(QLinear, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x).unsqueeze(0)\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Agent, self).__init__()\n",
    "        self.image_encoder = ImageEncoderCNN()\n",
    "        self.lstm = nn.LSTMCell(input_size=hidden_size, hidden_size=hidden_size)\n",
    "        self.language_encoder_mlp = LanguageEncoderMLP(input_size=44, hidden_size=hidden_size)\n",
    "        self.keyvalue_linear = KVLinear(hidden_size)\n",
    "        self.query_linear = QLinear(input_size=hidden_size, hidden_size=hidden_size)\n",
    "        self.attention = nn.MultiheadAttention(hidden_size, num_heads=1)\n",
    "        self.action_value_linear = nn.Linear(hidden_size, 4)\n",
    "        self.value_linear = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, vt, lt,lstm_hiden_state):\n",
    "        image_features = self.image_encoder(vt)\n",
    "        # image_features = image_features.unsqueeze(0)  # Add sequence dimension\n",
    "        lstm_output = self.lstm(image_features,lstm_hiden_state)\n",
    "        language_features = self.language_encoder_mlp(lt)\n",
    "        keys, values = self.keyvalue_linear(lstm_output[0])\n",
    "        querys = self.query_linear(language_features)\n",
    "        querys = querys.unsqueeze(0)\n",
    "        keys = keys.unsqueeze(0)\n",
    "        values = values.unsqueeze(0)\n",
    "        context_vector,_= self.attention(querys, keys, values)\n",
    "        context_vector = context_vector.squeeze(0)\n",
    "        actions = self.action_value_linear(context_vector)\n",
    "        value = self.value_linear(context_vector)\n",
    "        return actions, value, lstm_output \n",
    "    \n",
    "    def save(self, episode, best_score, ALG_NAME, ENV_ID):\n",
    "        path = os.path.join(r'C:\\Users\\linzj\\Desktop\\model', '_'.join([ALG_NAME, ENV_ID]))\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        torch.save(self.state_dict(), os.path.join(path, f'agent_{episode}_{best_score}.pt'))\n",
    "\n",
    "    def load(self, episode, ALG_NAME, ENV_ID):\n",
    "        path = os.path.join(r'C:\\Users\\linzj\\Desktop\\model', '_'.join([ALG_NAME, ENV_ID]))\n",
    "        self.load_state_dict(torch.load(os.path.join(path, f'agent_{episode}.pt')))    \n",
    "\n",
    "    def count_trainable_parameters(self):\n",
    "        print(\"Trainable parameters in each module:\")\n",
    "        for name, module in self.named_modules():\n",
    "            total_params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "            print(f\"{name}: {total_params}\")\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters in each module:\n",
      ": 1859077\n",
      "image_encoder: 859392\n",
      "image_encoder.conv: 56320\n",
      "image_encoder.conv.0: 896\n",
      "image_encoder.conv.1: 0\n",
      "image_encoder.conv.2: 18496\n",
      "image_encoder.conv.3: 0\n",
      "image_encoder.conv.4: 36928\n",
      "image_encoder.conv.5: 0\n",
      "image_encoder.fc: 803072\n",
      "lstm: 526336\n",
      "language_encoder_mlp: 11520\n",
      "language_encoder_mlp.fc1: 11520\n",
      "keyvalue_linear: 131584\n",
      "keyvalue_linear.key: 65792\n",
      "keyvalue_linear.value: 65792\n",
      "query_linear: 65792\n",
      "query_linear.fc: 65792\n",
      "attention: 263168\n",
      "attention.out_proj: 65792\n",
      "action_value_linear: 1028\n",
      "value_linear: 257\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(256)\n",
    "agent.count_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "num_words = 44  # Number of unique words in the vocabulary\n",
    "lstm_hidden_dim = 256\n",
    "num_actions = 4\n",
    "\n",
    "# (3,128,128) --> (64,16,16)\n",
    "class ImageEncoderCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageEncoderCNN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=3, padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc = nn.Linear(64 * 7* 7, 256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = F.relu(self.fc(x))\n",
    "        return x\n",
    "\n",
    "class LanguageEncoderMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LanguageEncoderMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.fc1(x)\n",
    "        return x1\n",
    "\n",
    "class KVLinear(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(KVLinear, self).__init__()\n",
    "        self.key = nn.Linear(hidden_size, hidden_size)\n",
    "        self.value = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.key(x)\n",
    "        values = self.value(x)\n",
    "        return keys, values\n",
    "\n",
    "class QLinear(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(QLinear, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x).unsqueeze(0)\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Agent, self).__init__()\n",
    "        self.image_encoder = ImageEncoderCNN()\n",
    "        self.language_encoder_mlp = LanguageEncoderMLP(input_size=44, hidden_size=hidden_size)\n",
    "        self.keyvalue_linear = KVLinear(hidden_size)\n",
    "        self.query_linear = QLinear(input_size=hidden_size, hidden_size=hidden_size)\n",
    "        self.attention = nn.MultiheadAttention(hidden_size, num_heads=1)\n",
    "        self.action_value_linear = nn.Linear(hidden_size, 4)\n",
    "        self.value_linear = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, vt, lt):\n",
    "        image_features = self.image_encoder(vt)\n",
    "        # image_features = image_features.unsqueeze(0)  # Add sequence dimension\n",
    "        # lstm_output = self.lstm(image_features,lstm_hiden_state)\n",
    "        language_features = self.language_encoder_mlp(lt)\n",
    "        keys, values = self.keyvalue_linear(image_features)\n",
    "        querys = self.query_linear(language_features)\n",
    "        querys = querys.unsqueeze(0)\n",
    "        keys = keys.unsqueeze(0)\n",
    "        values = values.unsqueeze(0)\n",
    "        context_vector,_= self.attention(querys, keys, values)\n",
    "        context_vector = context_vector.squeeze(0)\n",
    "        actions = self.action_value_linear(context_vector)\n",
    "        value = self.value_linear(context_vector)\n",
    "        return actions, value \n",
    "    \n",
    "    def save(self, episode, best_score, ALG_NAME, ENV_ID):\n",
    "        path = os.path.join(r'C:\\Users\\linzj\\Desktop\\model', '_'.join([ALG_NAME, ENV_ID]))\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        torch.save(self.state_dict(), os.path.join(path, f'agent_{episode}_{best_score}.pt'))\n",
    "\n",
    "    def load(self, episode, ALG_NAME, ENV_ID):\n",
    "        path = os.path.join(r'C:\\Users\\linzj\\Desktop\\model', '_'.join([ALG_NAME, ENV_ID]))\n",
    "        self.load_state_dict(torch.load(os.path.join(path, f'agent_{episode}.pt')))    \n",
    "\n",
    "    def count_trainable_parameters(self):\n",
    "        print(\"Trainable parameters in each module:\")\n",
    "        for name, module in self.named_modules():\n",
    "            total_params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "            print(f\"{name}: {total_params}\")\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters in each module:\n",
      ": 1332741\n",
      "image_encoder: 859392\n",
      "image_encoder.conv: 56320\n",
      "image_encoder.conv.0: 896\n",
      "image_encoder.conv.1: 0\n",
      "image_encoder.conv.2: 18496\n",
      "image_encoder.conv.3: 0\n",
      "image_encoder.conv.4: 36928\n",
      "image_encoder.conv.5: 0\n",
      "image_encoder.fc: 803072\n",
      "language_encoder_mlp: 11520\n",
      "language_encoder_mlp.fc1: 11520\n",
      "keyvalue_linear: 131584\n",
      "keyvalue_linear.key: 65792\n",
      "keyvalue_linear.value: 65792\n",
      "query_linear: 65792\n",
      "query_linear.fc: 65792\n",
      "attention: 263168\n",
      "attention.out_proj: 65792\n",
      "action_value_linear: 1028\n",
      "value_linear: 257\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(256)\n",
    "agent.count_trainable_parameters()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
