{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlagents\n",
    "from mlagents_envs.environment import UnityEnvironment as UE\n",
    "import numpy as np\n",
    "from mlagents_envs.environment import ActionTuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env =  UE(file_name=\"stage0_160523\\stage0_copy\",seed=1,side_channels=[])\n",
    "env_train =  UE(file_name=r\"C:\\Users\\linzj\\Desktop\\environment\\S2_train\\\\build\",seed=1,side_channels=[],worker_id=6,no_graphics = False)\n",
    "env_train.reset()\n",
    "# env_test =  UE(file_name=r\"C:\\Users\\linzj\\Desktop\\environment\\S2_test\\\\build\",seed=1,side_channels=[],worker_id=5,no_graphics = False)\n",
    "# env_test.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "vision_output_dim = 3136\n",
    "num_words = 35  # Number of unique words in the vocabulary\n",
    "language_output_dim = 128\n",
    "embedding_dim = 128\n",
    "mixing_dim = 256\n",
    "lstm_hidden_dim = 256\n",
    "num_actions = 4\n",
    "\n",
    "# (3,128,128) --> (64,7,7) = 3136 (3-layer CNN)\n",
    "class VisualModule(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(VisualModule, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=3, padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # self.conv = nn.Sequential(\n",
    "        #     nn.Conv2d(3, 32, kernel_size=5, stride=2, padding=2),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(32, 64, kernel_size=5, stride=2, padding=2),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(128, 64, kernel_size=5, stride=2, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        # )\n",
    "\n",
    "    def forward(self, vt):\n",
    "        encoded_vt = self.conv(vt)\n",
    "        return encoded_vt.view(vt.size(0), -1).squeeze()\n",
    "\n",
    "# one-hot encoding [0 0 1 0 0] --> 128 dimensional embedding (FF)\n",
    "# S1:5 S2:5 S3:11 S4:9 --> 30 + 5 (noun) = 35 in total\n",
    "class LanguageModule(nn.Module): \n",
    "    def __init__(self, num_words, embedding_dim):\n",
    "        super(LanguageModule, self).__init__()\n",
    "        self.embedding = nn.Linear(num_words, embedding_dim)\n",
    "        # self.embedding = nn.Sequential(\n",
    "        #             nn.Linear(num_words, embedding_dim),\n",
    "        #             nn.ReLU())\n",
    "\n",
    "    def forward(self, lt):\n",
    "        embedded_lt = self.embedding(lt)\n",
    "        return embedded_lt\n",
    "\n",
    "# 3136(vision) + 128 (language) --> 256 dimensional embedding (FF)\n",
    "class MixingModule(nn.Module):\n",
    "    def __init__(self, vision_output_dim, language_output_dim, mixing_dim):\n",
    "        super(MixingModule, self).__init__()\n",
    "        self.linear = nn.Linear(vision_output_dim + language_output_dim, mixing_dim)\n",
    "\n",
    "        # self.linear = nn.Sequential(\n",
    "        #             nn.Linear(vision_output_dim + language_output_dim, mixing_dim),\n",
    "        #             nn.ReLU())\n",
    "\n",
    "    def forward(self, vision_output, language_output):\n",
    "        combined_output = torch.cat((vision_output, language_output), dim=0)\n",
    "        mixed_output = self.linear(combined_output)\n",
    "        return mixed_output\n",
    "\n",
    "class LSTMModule(nn.Module):\n",
    "    def __init__(self,mixing_dim,lstm_hidden_dim):\n",
    "        super(LSTMModule, self).__init__()\n",
    "        self.lstm = nn.LSTMCell(mixing_dim, lstm_hidden_dim)\n",
    "    \n",
    "    def forward(self,mixed_output,lstm_hidden_state):\n",
    "        lstm_hidden_state = self.lstm(mixed_output, lstm_hidden_state) \n",
    "        # lstm_output = lstm_hidden_state[0] # output is (hidden_state,cell_state), we need hidden state, shape (1,256)\n",
    "        return lstm_hidden_state\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, num_words, embedding_dim, vision_output_dim, language_output_dim, mixing_dim, lstm_hidden_dim,num_actions):\n",
    "        super(Agent, self).__init__()\n",
    "        self.language_module = LanguageModule(num_words, embedding_dim)\n",
    "        self.visual_module = VisualModule()\n",
    "        self.mixing_module = MixingModule(vision_output_dim, language_output_dim, mixing_dim)\n",
    "        self.lstm_module = LSTMModule(mixing_dim, lstm_hidden_dim)\n",
    "        self.action_predictor = nn.Linear(lstm_hidden_dim, num_actions)\n",
    "        self.value_estimator = nn.Linear(lstm_hidden_dim, 1)\n",
    "\n",
    "    def forward(self, vt, lt, lstm_hidden_state):\n",
    "        vision_output = self.visual_module(vt)\n",
    "        language_output = self.language_module(lt)\n",
    "        mixed_output = self.mixing_module(vision_output, language_output).unsqueeze(0)\n",
    "        lstm_output = self.lstm_module(mixed_output,lstm_hidden_state)\n",
    "        action_probs = self.action_predictor(lstm_output[0]) \n",
    "        value_estimate = self.value_estimator(lstm_output[0])\n",
    "        return action_probs,value_estimate,lstm_output\n",
    "        \n",
    "        \n",
    "    def save(self, episode, ALG_NAME, ENV_ID):\n",
    "        path = os.path.join(r'C:\\Users\\linzj\\Desktop\\model_before', '_'.join([ALG_NAME, ENV_ID]))\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        torch.save(self.state_dict(), os.path.join(path, f'agent_{episode}.pt'))\n",
    "\n",
    "    def load(self, episode, ALG_NAME, ENV_ID):\n",
    "        path = os.path.join(r'C:\\Users\\linzj\\Desktop\\model_before', '_'.join([ALG_NAME, ENV_ID]))\n",
    "        self.load_state_dict(torch.load(os.path.join(path, f'agent_{episode}.pt')))    \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(agent,test_episode,test_episode_reward,test_average_reward,test_steps,test_actor_loss,test_critic_loss,test_entropy_loss,test_total_loss):\n",
    "    env = env_test\n",
    "    TEST_EPISODES = 100\n",
    "    tracked_agent = -1\n",
    "    entropy_term = 0\n",
    "    for episode in range(TEST_EPISODES):\n",
    "        test_episode += 1\n",
    "        t0 = time.time()\n",
    "        episode_reward = 0\n",
    "        # env.reset()\n",
    "        behavior_name=list(env.behavior_specs)[0]\n",
    "        spec=env.behavior_specs[behavior_name]\n",
    "        # state = env.reset().astype(np.float32)\n",
    "        STEPS = 0\n",
    "        decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "        # state -- vt, lt, lstm\n",
    "        vt = torch.tensor(decision_steps.obs[0]).reshape(1,3,128,128).to(device)\n",
    "        index1 = int(decision_steps.obs[1][0][0])\n",
    "        index2 = int(decision_steps.obs[1][0][1])+5\n",
    "        # print(f'TEST: ---{hashmap[index2]} {hashmap[index1]}---')\n",
    "        # 0-capsule,1-cube,2-cylinder,3-prism,4-sphere \n",
    "        lt = torch.zeros(35).to(device)\n",
    "        lt[index1],lt[index2] = 1,1\n",
    "        lstm_hidden_state = (torch.zeros(1, lstm_hidden_dim).to(device), torch.zeros(1, lstm_hidden_dim).to(device))\n",
    "        done = False\n",
    "        while True:\n",
    "\n",
    "            # Need to use when calculating the loss\n",
    "            log_probs = []\n",
    "            # values = []\n",
    "            values = torch.empty(0).to(device)\n",
    "            rewards = []\n",
    "\n",
    "            \n",
    "            lstm_hidden_state = tuple(tensor.detach() for tensor in lstm_hidden_state)\n",
    "            STEPS += 1\n",
    "            policy_dist, value, lstm_hidden_state = agent(vt,lt,lstm_hidden_state)\n",
    "            # value = value.detach()\n",
    "            dist = F.softmax(policy_dist.detach(),dim=1).cpu().numpy()\n",
    "            \n",
    "\n",
    "            action_dist = Categorical(F.softmax(policy_dist.detach(),dim=1))\n",
    "            # action_dist = Categorical(F.softmax(policy_dist,dim=1))\n",
    "            action = action_dist.sample() # sample an action from action_dist\n",
    "            action_onehot = F.one_hot(torch.tensor(action),num_actions).cpu()\n",
    "            \n",
    "            log_prob = torch.log(F.softmax(policy_dist,dim=1)[0][action])\n",
    "            # log_prob = torch.log(F.softmax(policy_dist,dim=1)[0][action])\n",
    "            # entropy = -np.sum(np.mean(dist)* np.log(dist))\n",
    "            entropy = F.cross_entropy(policy_dist.detach(), action)\n",
    "\n",
    "            discrete_actions = np.array(action_onehot).reshape(1,4)*speed\n",
    "            action_tuple = ActionTuple()\n",
    "            action_tuple.add_discrete(discrete_actions)\n",
    "            env.set_actions(behavior_name,action_tuple)\n",
    "            env.step()\n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "\n",
    "            if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "                tracked_agent = decision_steps.agent_id[0]\n",
    "                # print(tracked_agent)\n",
    "\n",
    "            if tracked_agent in terminal_steps: # roll over or hit the target\n",
    "                # print('TEST: Agent in terminal steps')\n",
    "                done = True\n",
    "                reward = terminal_steps[tracked_agent].reward\n",
    "                if reward > 0:\n",
    "                    pass\n",
    "                else: reward = -1 # roll over or other unseen conditions\n",
    "\n",
    "                # print(f'TEST: Terminal Step reward: {reward}')\n",
    "\n",
    "            elif tracked_agent in decision_steps: # the agent which requires action\n",
    "                reward = decision_steps[tracked_agent].reward\n",
    "                # print(f'Decision Step reward: {reward}')\n",
    "                # if reward<0:\n",
    "                #     print(f'TEST: Decision Step reward: {reward}')\n",
    "            if STEPS >= MAX_STEPS:\n",
    "                reward = -10\n",
    "                # print(f'TEST: Max Step Reward: {reward}')\n",
    "                env.reset()\n",
    "                done = True\n",
    "            # if STEPS % 100 == 0:\n",
    "            #     print (f'TEST: Step: {STEPS}')\n",
    "\n",
    "            episode_reward = episode_reward + reward\n",
    "\n",
    "            rewards.append(reward)\n",
    "            # values.append(value)\n",
    "            values = torch.cat((values, value), dim=0)\n",
    "            log_probs.append(log_prob)\n",
    "            entropy_term = entropy_term + entropy\n",
    "            vt_new = torch.tensor(decision_steps.obs[0]).reshape(1,3,128,128).to(device)\n",
    "            vt = vt_new\n",
    "\n",
    "            if done:\n",
    "                # _, Qval,_ = agent(vt_new,lt,lstm_hidden_state)\n",
    "                # Qval = Qval.detach()\n",
    "                break\n",
    "            \n",
    "            \n",
    "        discounted_rewards = np.zeros_like(values.cpu().detach().numpy())\n",
    "        cumulative = 0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            cumulative = rewards[t] + LAM * cumulative # Monte Carlo\n",
    "            discounted_rewards[t] = cumulative\n",
    "        # print(f'rewards:{rewards}, discounted_rewards:{discounted_rewards}')\n",
    "        # Advantage Actor Critic\n",
    "\n",
    "        # Qvals[-1] = rewards[t] + LAM * Qval      or       Qvals[-1] = rewards[t]                   \n",
    "        # for t in range(len(rewards)-1):\n",
    "        #         Qvals[t] = rewards[t] + LAM * values[t+1]\n",
    "        \n",
    "        # r_(t+1) = R(s_t|a_t)--> reward[t]        a_t, V_t = agent(s_t)\n",
    "        # A_t = r_(t+1) + LAM * V_(t+1) - V_t \n",
    "        #     = Q_t - V_t\n",
    "        \n",
    "        # Monte Carlo Advantage = reward + LAM * cumulative_reward\n",
    "        # Actor_loss = -log(pai(s_t|a_t))*A_t\n",
    "        # Critic_loss = A_t.pow(2) *0.5\n",
    "        # Entropy_loss = -F.entropy(pai(St),index) * 0.001\n",
    "\n",
    "        # entropy = -np.sum(np.mean(dist) * np.log(dist))\n",
    "        \n",
    "        #update actor critic\n",
    "        \n",
    "        # values = torch.FloatTensor(values).requires_grad_(True).to(device)\n",
    "        discounted_rewards = torch.FloatTensor(discounted_rewards.astype(np.float32)).to(device)\n",
    "        log_probs = torch.stack(log_probs)\n",
    "        advantage = discounted_rewards - values\n",
    "        actor_loss = (-log_probs * advantage).mean()\n",
    "        critic_loss = 0.5 * torch.square(advantage).mean()\n",
    "        entropy_term /= STEPS\n",
    "        entropy_loss = -0.1 * entropy_term\n",
    "        ac_loss = actor_loss + critic_loss + entropy_loss\n",
    "        test_episode_reward.append(float(episode_reward))\n",
    "        test_steps.append(STEPS)\n",
    "        test_actor_loss.append(float(actor_loss))\n",
    "        test_critic_loss.append(float(critic_loss))\n",
    "        test_entropy_loss.append(float(entropy_loss))\n",
    "        test_total_loss.append(float(ac_loss))\n",
    "\n",
    "        if test_episode >= 100:\n",
    "            avg_score = np.mean(test_episode_reward[-100:])\n",
    "            if avg_score > 9: agent.save(f'finish_{episode}', ALG_NAME, ENV_ID)\n",
    "            test_average_reward.append(avg_score)\n",
    "            print('Testing  | Episode: {}/{}  | Episode Reward: {:.0f}  | Average Reward {:.2f}  | Actor loss: {:.2f} | Critic loss: {:.2f} | Entropy loss: {:.4f}  | Total Loss: {:.2f} | Total Steps: {}' \\\n",
    "                .format(episode + 1, TEST_EPISODES, episode_reward, avg_score, actor_loss, critic_loss,entropy_loss,  ac_loss, STEPS))\n",
    "        else:  print('Testing  | Episode: {}/{}  | Episode Reward: {:.0f}  | Actor loss: {:.2f} | Critic loss: {:.2f} | Entropy loss: {:.4f}  | Total Loss: {:.2f} | Total Steps: {}' \\\n",
    "                .format(episode + 1, TEST_EPISODES, episode_reward, actor_loss, critic_loss, entropy_loss,  ac_loss, STEPS))\n",
    "    return test_episode,test_episode_reward,test_average_reward,test_steps,test_actor_loss,test_critic_loss,test_entropy_loss,test_total_loss\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training  | Episode: 10300/30000  | Episode Reward: 10  | Average Reward 9.61  | Actor loss: -0.10 | Critic loss: 1.59 | Entropy loss: -0.0006  | Total Loss: 1.49 | Total Steps: 37\n",
      "Training  | Episode: 10301/30000  | Episode Reward: 10  | Average Reward 9.61  | Actor loss: 0.17 | Critic loss: 0.32 | Entropy loss: -0.0007  | Total Loss: 0.49 | Total Steps: 11\n",
      "Training  | Episode: 10302/30000  | Episode Reward: 10  | Average Reward 9.61  | Actor loss: -0.06 | Critic loss: 0.19 | Entropy loss: -0.0010  | Total Loss: 0.13 | Total Steps: 10\n",
      "Training  | Episode: 10303/30000  | Episode Reward: 4  | Average Reward 9.55  | Actor loss: -1.11 | Critic loss: 11.08 | Entropy loss: -0.0121  | Total Loss: 9.96 | Total Steps: 89\n",
      "Training  | Episode: 10304/30000  | Episode Reward: 10  | Average Reward 9.58  | Actor loss: 0.01 | Critic loss: 0.12 | Entropy loss: -0.0003  | Total Loss: 0.12 | Total Steps: 12\n",
      "Training  | Episode: 10305/30000  | Episode Reward: 10  | Average Reward 9.58  | Actor loss: 0.00 | Critic loss: 0.02 | Entropy loss: -0.0000  | Total Loss: 0.02 | Total Steps: 6\n",
      "Training  | Episode: 10306/30000  | Episode Reward: 10  | Average Reward 9.58  | Actor loss: 0.07 | Critic loss: 0.18 | Entropy loss: -0.0008  | Total Loss: 0.25 | Total Steps: 9\n",
      "Training  | Episode: 10307/30000  | Episode Reward: 10  | Average Reward 9.58  | Actor loss: 0.05 | Critic loss: 0.25 | Entropy loss: -0.0006  | Total Loss: 0.30 | Total Steps: 10\n",
      "Training  | Episode: 10308/30000  | Episode Reward: 10  | Average Reward 9.58  | Actor loss: 0.00 | Critic loss: 0.03 | Entropy loss: -0.0000  | Total Loss: 0.03 | Total Steps: 6\n",
      "Training  | Episode: 10309/30000  | Episode Reward: 10  | Average Reward 9.61  | Actor loss: -0.02 | Critic loss: 0.10 | Entropy loss: -0.0008  | Total Loss: 0.08 | Total Steps: 14\n",
      "Training  | Episode: 10310/30000  | Episode Reward: 10  | Average Reward 9.70  | Actor loss: 0.01 | Critic loss: 0.08 | Entropy loss: -0.0001  | Total Loss: 0.09 | Total Steps: 8\n",
      "Training  | Episode: 10311/30000  | Episode Reward: 10  | Average Reward 9.70  | Actor loss: -0.00 | Critic loss: 0.04 | Entropy loss: -0.0000  | Total Loss: 0.04 | Total Steps: 6\n",
      "Training  | Episode: 10312/30000  | Episode Reward: 10  | Average Reward 9.70  | Actor loss: -0.23 | Critic loss: 0.27 | Entropy loss: -0.0021  | Total Loss: 0.05 | Total Steps: 14\n",
      "Training  | Episode: 10313/30000  | Episode Reward: 10  | Average Reward 9.70  | Actor loss: -0.04 | Critic loss: 0.07 | Entropy loss: -0.0009  | Total Loss: 0.03 | Total Steps: 10\n",
      "Training  | Episode: 10314/30000  | Episode Reward: 10  | Average Reward 9.70  | Actor loss: 0.00 | Critic loss: 0.18 | Entropy loss: -0.0000  | Total Loss: 0.18 | Total Steps: 8\n",
      "Training  | Episode: 10315/30000  | Episode Reward: 10  | Average Reward 9.70  | Actor loss: 0.00 | Critic loss: 0.03 | Entropy loss: -0.0000  | Total Loss: 0.03 | Total Steps: 6\n",
      "Training  | Episode: 10316/30000  | Episode Reward: 7  | Average Reward 9.67  | Actor loss: -0.13 | Critic loss: 4.47 | Entropy loss: -0.0007  | Total Loss: 4.33 | Total Steps: 41\n",
      "Training  | Episode: 10317/30000  | Episode Reward: 10  | Average Reward 9.67  | Actor loss: 0.00 | Critic loss: 0.01 | Entropy loss: -0.0000  | Total Loss: 0.01 | Total Steps: 6\n",
      "Training  | Episode: 10318/30000  | Episode Reward: 10  | Average Reward 9.67  | Actor loss: -0.43 | Critic loss: 1.27 | Entropy loss: -0.0040  | Total Loss: 0.83 | Total Steps: 34\n",
      "Training  | Episode: 10319/30000  | Episode Reward: 10  | Average Reward 9.67  | Actor loss: 0.00 | Critic loss: 0.61 | Entropy loss: -0.0004  | Total Loss: 0.61 | Total Steps: 31\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-c06de99eb3b3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m                     \u001b[0maction_dist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy_dist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m                     \u001b[1;31m# action_dist = Categorical(F.softmax(policy_dist,dim=1))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m                     \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction_dist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# sample an action from action_dist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\linzj\\anaconda3\\envs\\rl\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1678\u001b[0m         \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"softmax\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1679\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1680\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1681\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1682\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import time\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "entropy_term = 0\n",
    "# add arguments in command --train/test\n",
    "# parser = argparse.ArgumentParser(description='Train or test neural net motor controller.')\n",
    "# parser.add_argument('--train', dest='train', action='store_true', default=False)\n",
    "# parser.add_argument('--test', dest='test', action='store_true', default=True)\n",
    "# args = parser.parse_args()\n",
    "train = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device) \n",
    "S0_ALG_NAME = 'S2_final'\n",
    "S0_ENV_ID = '8'\n",
    "S0_episode = 52391\n",
    "ALG_NAME = 'S2_final'\n",
    "ENV_ID = '13'\n",
    "TRAIN_EPISODES = 30000  # number of overall episodes for training  # number of overall episodes for testing\n",
    "MAX_STEPS = 500  # maximum time step in one episode\n",
    "LAM = 0.95  # reward discount in TD error\n",
    "lr = 2.5e-5  #0.00005 \n",
    "speed = 3\n",
    "num_steps = 250 # the step for updating the network\n",
    "test_episode = 0\n",
    "if __name__ == '__main__':\n",
    "    agent = Agent(num_words, embedding_dim, vision_output_dim, language_output_dim, mixing_dim, lstm_hidden_dim,num_actions)\n",
    "    agent.load(S0_episode,S0_ALG_NAME,S0_ENV_ID)\n",
    "    agent.to(device)\n",
    "    optimizer = optim.RMSprop(agent.parameters(), lr=lr)\n",
    "    best_score = float('-inf')\n",
    "    hashmap = {\n",
    "        0: 'capsule',\n",
    "        1: 'cube',\n",
    "        2: 'cylinder',\n",
    "        3: 'prism',\n",
    "        4: 'sphere',\n",
    "        5: 'red',\n",
    "        6: 'green',\n",
    "        7: 'blue',\n",
    "        8: 'yellow',\n",
    "        9: 'black'}\n",
    "    if train:\n",
    "        entropy_term = 0\n",
    "        test_episode_reward = []\n",
    "        test_average_reward = []\n",
    "        test_steps = []\n",
    "        test_actor_loss = []\n",
    "        test_critic_loss = []\n",
    "        test_entropy_loss = []\n",
    "        test_total_loss = []\n",
    "        tracked_agent = -1\n",
    "        test_episode = 0\n",
    "        all_episode_reward = []\n",
    "        all_average_reward = []\n",
    "        all_steps = []\n",
    "        all_actor_loss = []\n",
    "        all_critic_loss = []\n",
    "        all_entropy_loss = []\n",
    "        all_total_loss = []\n",
    "        env = env_train\n",
    "        for episode in range(TRAIN_EPISODES):\n",
    "            t0 = time.time()\n",
    "            episode_reward = 0\n",
    "            # env.reset()\n",
    "            behavior_name=list(env.behavior_specs)[0]\n",
    "            spec=env.behavior_specs[behavior_name]\n",
    "            # state = env.reset().astype(np.float32)\n",
    "            STEPS = 0\n",
    "\n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "            # state -- vt, lt, lstm\n",
    "            vt = torch.tensor(decision_steps.obs[0]).reshape(1,3,128,128).to(device)\n",
    "            index1 = int(decision_steps.obs[1][0][0])\n",
    "            index2 = int(decision_steps.obs[1][0][1])+5\n",
    "            # print(f'---{hashmap[index2]} {hashmap[index1]}---')\n",
    "            # 0-capsule,1-cube,2-cylinder,3-prism,4-sphere \n",
    "            lt = torch.zeros(35).to(device)\n",
    "            lt[index1],lt[index2] = 1,1\n",
    "            lstm_hidden_state = (torch.zeros(1, lstm_hidden_dim).to(device), torch.zeros(1, lstm_hidden_dim).to(device))\n",
    "            done = False\n",
    "            while True:\n",
    "\n",
    "                # Need to use when calculating the loss\n",
    "                log_probs = []\n",
    "                # values = []\n",
    "                values = torch.empty(0).to(device)\n",
    "                rewards = []\n",
    "\n",
    "                for steps in range(num_steps):\n",
    "                    lstm_hidden_state = tuple(tensor.detach() for tensor in lstm_hidden_state)\n",
    "                    STEPS += 1\n",
    "                    policy_dist, value, lstm_hidden_state = agent(vt,lt,lstm_hidden_state)\n",
    "                    # value = value.detach()\n",
    "                    dist = F.softmax(policy_dist.detach(),dim=1).cpu().numpy()\n",
    "                    \n",
    "\n",
    "                    action_dist = Categorical(F.softmax(policy_dist.detach(),dim=1))\n",
    "                    # action_dist = Categorical(F.softmax(policy_dist,dim=1))\n",
    "                    action = action_dist.sample() # sample an action from action_dist\n",
    "                    action_onehot = F.one_hot(torch.tensor(action),num_actions).cpu()\n",
    "                    \n",
    "                    log_prob = torch.log(F.softmax(policy_dist,dim=1)[0][action])\n",
    "                    # log_prob = torch.log(F.softmax(policy_dist,dim=1)[0][action])\n",
    "                    # entropy = -np.sum(np.mean(dist)* np.log(dist))\n",
    "                    entropy = F.cross_entropy(policy_dist.detach(), action)\n",
    "\n",
    "                    discrete_actions = np.array(action_onehot).reshape(1,4)*speed\n",
    "                    action_tuple = ActionTuple()\n",
    "                    action_tuple.add_discrete(discrete_actions)\n",
    "                    env.set_actions(behavior_name,action_tuple)\n",
    "                    env.step()\n",
    "                    decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "\n",
    "                    if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "                        tracked_agent = decision_steps.agent_id[0]\n",
    "                        # print(tracked_agent)\n",
    "\n",
    "                    if tracked_agent in terminal_steps: # roll over or hit the target\n",
    "                        # print('Agent in terminal steps')\n",
    "                        done = True\n",
    "                        reward = terminal_steps[tracked_agent].reward\n",
    "                        if reward > 0:\n",
    "                            pass\n",
    "                        else: reward = -1 # roll over or other unseen conditions\n",
    "\n",
    "                        # print(f'Terminal Step reward: {reward}')\n",
    "\n",
    "                    elif tracked_agent in decision_steps: # the agent which requires action\n",
    "                        reward = decision_steps[tracked_agent].reward\n",
    "                        # print(f'Decision Step reward: {reward}')\n",
    "                        # if reward<0:\n",
    "                        #     print(f'Decision Step reward: {reward}')\n",
    "                    if STEPS >= MAX_STEPS:\n",
    "                        reward = -10\n",
    "                        # print(f'Max Step Reward: {reward}')\n",
    "                        env.reset()\n",
    "                        done = True\n",
    "                    # if STEPS % num_steps == 0:\n",
    "                    #     print (f'Step: {STEPS}')\n",
    "\n",
    "                    episode_reward = episode_reward + reward\n",
    "\n",
    "                    rewards.append(reward)\n",
    "                    # values.append(value)\n",
    "                    values = torch.cat((values, value), dim=0)\n",
    "                    log_probs.append(log_prob)\n",
    "                    entropy_term = entropy_term + entropy\n",
    "                    vt_new = torch.tensor(decision_steps.obs[0]).reshape(1,3,128,128).to(device)\n",
    "                    vt = vt_new\n",
    "\n",
    "                    if done or steps == num_steps-1:\n",
    "                        # _, Qval,_ = agent(vt_new,lt,lstm_hidden_state)\n",
    "                        # Qval = Qval.detach()\n",
    "                        break\n",
    "                \n",
    "                \n",
    "                discounted_rewards = np.zeros_like(values.cpu().detach().numpy())\n",
    "                cumulative = 0\n",
    "                for t in reversed(range(len(rewards))):\n",
    "                    cumulative = rewards[t] + LAM * cumulative # Monte Carlo\n",
    "                    discounted_rewards[t] = cumulative\n",
    "                # print(f'rewards:{rewards}, discounted_rewards:{discounted_rewards}')\n",
    "                # Advantage Actor Critic\n",
    "\n",
    "                # Qvals[-1] = rewards[t] + LAM * Qval      or       Qvals[-1] = rewards[t]                   \n",
    "                # for t in range(len(rewards)-1):\n",
    "                #         Qvals[t] = rewards[t] + LAM * values[t+1]\n",
    "                \n",
    "                # r_(t+1) = R(s_t|a_t)--> reward[t]        a_t, V_t = agent(s_t)\n",
    "                # A_t = r_(t+1) + LAM * V_(t+1) - V_t \n",
    "                #     = Q_t - V_t\n",
    "                \n",
    "                # Monte Carlo Advantage = reward + LAM * cumulative_reward\n",
    "                # Actor_loss = -log(pai(s_t|a_t))*A_t\n",
    "                # Critic_loss = A_t.pow(2) *0.5\n",
    "                # Entropy_loss = -F.entropy(pai(St),index) * 0.001\n",
    "\n",
    "                # entropy = -np.sum(np.mean(dist) * np.log(dist))\n",
    "                \n",
    "                #update actor critic\n",
    "                \n",
    "                # values = torch.FloatTensor(values).requires_grad_(True).to(device)\n",
    "                discounted_rewards = torch.FloatTensor(discounted_rewards.astype(np.float32)).to(device)\n",
    "                log_probs = torch.stack(log_probs)\n",
    "                advantage = discounted_rewards - values\n",
    "                actor_loss = (-log_probs * advantage).mean()\n",
    "                critic_loss = 0.5 * torch.square(advantage).mean()\n",
    "                entropy_term /= num_steps\n",
    "                entropy_loss = -0.1 * entropy_term\n",
    "                ac_loss = actor_loss + critic_loss + entropy_loss\n",
    "                # ac_loss = values.mean()\n",
    "                optimizer.zero_grad()\n",
    "                ac_loss.backward()\n",
    "                optimizer.step()\n",
    "                # for name, param in agent.named_parameters():\n",
    "                #     if param.grad is not None:\n",
    "                #         print(name, param.grad)\n",
    "                #     else:\n",
    "                #         print(name, \"gradients not computed\")\n",
    "                # for name, param in agent.named_parameters():\n",
    "                #     if name == 'value_estimator.weight':\n",
    "                #         print(name, param)\n",
    "                \n",
    "                \n",
    "                if done: break\n",
    "\n",
    "\n",
    "            all_episode_reward.append(float(episode_reward))\n",
    "            all_steps.append(STEPS)\n",
    "            all_actor_loss.append(float(actor_loss))\n",
    "            all_critic_loss.append(float(critic_loss))\n",
    "            all_entropy_loss.append(float(entropy_loss))\n",
    "            all_total_loss.append(float(ac_loss))\n",
    "            if episode >= 100:\n",
    "                avg_score = np.mean(all_episode_reward[-100:])\n",
    "                all_average_reward.append(avg_score)\n",
    "                if avg_score > best_score:\n",
    "                    best_score = avg_score\n",
    "                    agent.save(episode, ALG_NAME, ENV_ID)\n",
    "                    print(f'-----The best score for averaging previous 100 episode reward is {best_score}. Model has been saved-----')\n",
    "                print('Training  | Episode: {}/{}  | Episode Reward: {:.0f}  | Average Reward {:.2f}  | Actor loss: {:.2f} | Critic loss: {:.2f} | Entropy loss: {:.4f}  | Total Loss: {:.2f} | Total Steps: {}' \\\n",
    "                    .format(episode + 1, TRAIN_EPISODES, episode_reward, avg_score, actor_loss, critic_loss,entropy_loss,  ac_loss, STEPS))\n",
    "            else:  print('Training  | Episode: {}/{}  | Episode Reward: {:.0f}  | Actor loss: {:.2f} | Critic loss: {:.2f} | Entropy loss: {:.4f}  | Total Loss: {:.2f} | Total Steps: {}' \\\n",
    "                    .format(episode + 1, TRAIN_EPISODES, episode_reward, actor_loss, critic_loss, entropy_loss,  ac_loss, STEPS))\n",
    "            if episode%5000 == 0:\n",
    "                    agent.save(episode, ALG_NAME, ENV_ID)\n",
    "                    print(\"Model has been saved\")\n",
    "            # if (episode+1)%100 == 0:\n",
    "            #     test_episode,test_episode_reward,test_average_reward,test_steps,test_actor_loss,test_critic_loss,test_entropy_loss,test_total_loss = test(agent,test_episode,test_episode_reward,test_average_reward,test_steps,test_actor_loss,test_critic_loss,test_entropy_loss,test_total_loss)\n",
    "\n",
    "        print(all_average_reward)\n",
    "        agent.save(episode ,ALG_NAME, ENV_ID)\n",
    "        print(\"Model has been saved\")\n",
    "\n",
    "        data = {\n",
    "                    'all_average_reward': all_average_reward,\n",
    "                    'all_episode_reward': all_episode_reward,\n",
    "                    'all_actor_loss': all_actor_loss,\n",
    "                    'all_critic_loss': all_critic_loss,\n",
    "                    'all_entropy_loss': all_entropy_loss,\n",
    "                    'all_total_loss': all_total_loss,\n",
    "                    'all_steps': all_steps,\n",
    "                } \n",
    "        file_path = rf'C:\\Users\\linzj\\Desktop\\result\\{ALG_NAME}_{ENV_ID}_train.txt'\n",
    "        with open(file_path, 'w') as file:\n",
    "            json.dump(data, file)\n",
    "        \n",
    "        test_data = {\n",
    "                    'all_average_reward': test_average_reward,\n",
    "                    'all_episode_reward': test_episode_reward,\n",
    "                    'all_actor_loss': test_actor_loss,\n",
    "                    'all_critic_loss': test_critic_loss,\n",
    "                    'all_entropy_loss': test_entropy_loss,\n",
    "                    'all_total_loss': test_total_loss,\n",
    "                    'all_steps': test_steps,\n",
    "                } \n",
    "        file_path = rf'C:\\Users\\linzj\\Desktop\\result\\{ALG_NAME}_{ENV_ID}_test.txt'\n",
    "        with open(file_path, 'w') as file:\n",
    "            json.dump(test_data, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "            'all_average_reward': all_average_reward,\n",
    "            'all_episode_reward': all_episode_reward,\n",
    "            'all_actor_loss': all_actor_loss,\n",
    "            'all_critic_loss': all_critic_loss,\n",
    "            'all_entropy_loss': all_entropy_loss,\n",
    "            'all_total_loss': all_total_loss,\n",
    "            'all_steps': all_steps,\n",
    "        } \n",
    "file_path = rf'C:\\Users\\linzj\\Desktop\\result\\{ALG_NAME}_{ENV_ID}_train.txt'\n",
    "with open(file_path, 'w') as file:\n",
    "    json.dump(data, file)\n",
    "\n",
    "test_data = {\n",
    "            'all_average_reward': test_average_reward,\n",
    "            'all_episode_reward': test_episode_reward,\n",
    "            'all_actor_loss': test_actor_loss,\n",
    "            'all_critic_loss': test_critic_loss,\n",
    "            'all_entropy_loss': test_entropy_loss,\n",
    "            'all_total_loss': test_total_loss,\n",
    "            'all_steps': test_steps,\n",
    "        } \n",
    "file_path = rf'C:\\Users\\linzj\\Desktop\\result\\{ALG_NAME}_{ENV_ID}_test.txt'\n",
    "with open(file_path, 'w') as file:\n",
    "    json.dump(test_data, file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
